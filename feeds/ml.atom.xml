<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>寒玉 Blog - ml</title><link href="xixuebin.github.io/" rel="alternate"></link><link href="xixuebin.github.io/feeds/ml.atom.xml" rel="self"></link><id>xixuebin.github.io/</id><updated>2018-04-04T10:38:01+08:00</updated><entry><title>概率论(机器学习)-朴素贝叶斯公式</title><link href="xixuebin.github.io/ml-naive-bayes-2018-04-04-1038-ch.html" rel="alternate"></link><published>2018-04-04T10:38:01+08:00</published><updated>2018-04-04T10:38:01+08:00</updated><author><name>kevin.xi</name></author><id>tag:None,2018-04-04:xixuebin.github.io/ml-naive-bayes-2018-04-04-1038-ch.html</id><summary type="html">&lt;p&gt;朴素贝叶斯公式&lt;/p&gt;</summary><content type="html">&lt;hr&gt;
&lt;blockquote&gt;
&lt;p&gt;在机器学习中，朴素贝叶斯分类器是一个基于贝叶斯定理的比较简单的概率分类器，其中 naive（朴素）是指的对于模型中各个 feature（特征） 有强独立性的假设，并未将 feature 间的相关性纳入考虑中。
朴素贝叶斯分类器一个比较著名的应用是用于对垃圾邮件分类，通常用文字特征来识别垃圾邮件，是文本分类中比较常用的一种方法。朴素贝叶斯分类通过选择token（通常是邮件中的单词）来得到垃圾邮件和非垃圾邮件间的关联，再通过贝叶斯定理来计算概率从而对邮件进行分类。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;贝叶斯公式&lt;/h1&gt;
&lt;p&gt;&lt;code&gt;P(A|B) = P(B|A)*P(A)/P(B)&lt;/code&gt;&lt;/p&gt;
&lt;h2&gt;什么是贝叶斯推断&lt;/h2&gt;
&lt;p&gt;贝叶斯推断是一种统计学方法,用来估计统计量的某种特质.&lt;/p&gt;
&lt;h2&gt;朴素贝叶斯定理&lt;/h2&gt;
&lt;p&gt;设D是训练样本和相关联的类标的集合,其中训练样本属性集为:math:&lt;code&gt;A_\text{c} = (\pi/4) d^2&lt;/code&gt;.&lt;/p&gt;
&lt;h1&gt;参考&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://mp.weixin.qq.com/s/UXvSe_FYcS_s5HicMV6SUA"&gt;6步骤带你了解朴素贝叶斯分类器&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.cnblogs.com/pursued-deer/p/7783459.html"&gt;机器学习之朴素贝叶斯算法&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="ml"></category></entry><entry><title>概率论(机器学习)-贝叶斯公式</title><link href="xixuebin.github.io/ml-bayes-2018-03-22-1038-ch.html" rel="alternate"></link><published>2018-03-22T10:38:01+08:00</published><updated>2018-03-22T10:38:01+08:00</updated><author><name>kevin.xi</name></author><id>tag:None,2018-03-22:xixuebin.github.io/ml-bayes-2018-03-22-1038-ch.html</id><summary type="html">&lt;p&gt;贝叶斯公式&lt;/p&gt;</summary><content type="html">&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87"&gt;条件概率&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;贝叶斯定理是关于随机事件A和B的条件概率（或边缘概率）的一则定理,其中P(A|B)是在B发生的情况下A发生的可能性。贝叶斯定理也称贝叶斯推理.&lt;code&gt;P(A/B)=P(B/A)*P(A)/P(B)&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;条件概率&lt;/h1&gt;
&lt;p&gt;在事件B发生的基础上,A事假发生的概率,记为P(A|B)&lt;/p&gt;
&lt;p&gt;从贝叶斯的公式来看有三个要素:&lt;/p&gt;
&lt;h2&gt;先验概率&lt;/h2&gt;
&lt;p&gt;P(A)为先验概率,即在不知道事件B的前提下,对事件A的一个主观判断&lt;/p&gt;
&lt;h2&gt;可能函数&lt;/h2&gt;
&lt;p&gt;P(B|A)/P(B)称为可能函数,这是一个调整因子,即新信息带来的调整,使得先验概率更接近真实概率&lt;/p&gt;
&lt;h2&gt;后验概率&lt;/h2&gt;
&lt;p&gt;P(A|B)称为后验概率,即在B事件发生之后,我们对A事件的重新评估&lt;/p&gt;
&lt;p&gt;因此贝叶斯定理可以理解为:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;后验概率(新信息出现后A的概率) = 先验概率(A概率) * 可能函数(新信息带来的调整)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;贝叶斯的底层思想就是:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;如果能够掌握一个事情的全部信息,就可以计算出一个客观概率.如果决策信息不全,只能根据有限信息,尽可能做出一个好的预测.在主观判断的基础上,先估计一个值(后验概率).然后根据观察新的信息不断修正(可能函数).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;期望,方差,标准差,协方差&lt;/h2&gt;
&lt;p&gt;一个离散性随机变量的期望值是试验中每次可能的结果乘以其结果概率的总和&lt;/p&gt;
&lt;p&gt;掷一枚公平的六面骰子，其每次“点数”的期望值是3.5&lt;/p&gt;
&lt;p&gt;&lt;img alt="https://s1.ax1x.com/2018/03/22/9HQrFI.png" src="https://s1.ax1x.com/2018/03/22/9HQrFI.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;方差&lt;/code&gt;是用来衡量一组数据离散程度的统计量&lt;/p&gt;
&lt;p&gt;&lt;code&gt;标准差（standard deviation)&lt;/code&gt;等同于均方差（mean square error, MSE）。standard deviation英文译为“标准偏离”，事实上，标准差计算一组数据偏离均值的平均幅度，不管这组数据是样本数据还是总体数据&lt;/p&gt;
&lt;p&gt;&lt;em&gt;标准差是方差的平方根&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;协方差（Covariance）&lt;/code&gt;在概率论和统计学中用于衡量两个变量的总体误差。而方差是协方差的一种特殊情况，即当两个变量是相同的情况。&lt;/p&gt;
&lt;p&gt;从直观上来看，协方差表示的是两个变量总体误差的期望。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果两个变量的变化趋势一致，也就是说如果其中一个大于自身的期望值时另外一个也大于自身的期望值，那么两个变量之间的协方差就是正值；&lt;/li&gt;
&lt;li&gt;如果两个变量的变化趋势相反，即其中一个变量大于自身的期望值时另外一个却小于自身的期望值，那么两个变量之间的协方差就是负值。&lt;/li&gt;
&lt;li&gt;如果X与Y是统计独立的，那么二者之间的协方差就是0，因为两个独立的随机变量满足E[XY]=E[X]E[Y]。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;但是，反过来并不成立。即如果X与Y的协方差为0，二者并不一定是统计独立的。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;协方差Cov(X,Y)的度量单位是X的协方差乘以Y的协方差。而取决于协方差的相关性，是一个衡量线性独立的无量纲的数。
协方差为0的两个随机变量称为是不相关的&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;联合概率分布&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;联合概率分布&lt;/code&gt;简称联合分布，是两个及以上随机变量组成的随机变量的概率分布。根据随机变量的不同，联合概率分布的表示形式也不同。对于离散型随机变量，联合概率分布可以以列表的形式表示，也可以以函数的形式表示；对于连续型随机变量，联合概率分布通过非负函数的积分表示。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;联合概率分布&lt;/code&gt;分为&lt;code&gt;联合离散型概率分布&lt;/code&gt;和&lt;code&gt;联合连续型概率分布&lt;/code&gt;&lt;/p&gt;
&lt;h3&gt;离散型&lt;/h3&gt;
&lt;p&gt;对于二维离散随机向量，设X和Y都是离散型随机变量，{Xi}和 {Yj} 分别是X和Y的一切可能的几何，则X和Y的联合概率分布可以表示为如右图的列联表，也可以表示为如下的函数形式&lt;/p&gt;
&lt;h3&gt;连续型&lt;/h3&gt;
&lt;p&gt;对于二维连续随机向量，设X和Y为连续型随机变量，其联合概率分布，或连续型随机变量(X,Y)的概率分布F(x,y)通过一非负函数 f(x,y) &amp;gt; 0 的积分表示，称函数 f(x,y)为联合概率密度.&lt;/p&gt;
&lt;h2&gt;概率密度函数&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;连续型随机变量的概率密度函数（在不至于混淆时可以简称为密度函数）是一个描述这个随机变量的输出值，在某个确定的取值点附近的可能性的函数。而随机变量的取值落在某个区域之内的概率则为概率密度函数在这个区域上的积分。当概率密度函数存在的时候，累积分布函数是概率密度函数的积分。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;伯努利分布&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;伯努利分布&lt;/code&gt;亦称“零一分布”、“两点分布”。称随机变量X有伯努利分布, 参数为p(0&amp;lt;p&amp;lt;1),如果它分别以概率p和1-p取1和0为值。EX=p,DX=p(1-p)。伯努利试验成功的次数服从伯努利分布,参数p是试验成功的概率。伯努利分布是一个离散型机率分布，是N=1时二项分布的特殊情况.&lt;/p&gt;
&lt;h3&gt;二项式分布&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;二项式分布&lt;/code&gt;就是重复n次独立的伯努利试验。在每次试验中只有两种可能的结果，而且两种结果发生与否互相对立，并且相互独立，与其它各次试验结果无关，事件发生与否的概率在每一次独立试验中都保持不变，则这一系列试验总称为n重伯努利实验，当试验次数为1时，二项分布服从0-1分布。二项分布的极限分布为正态分布.&lt;/p&gt;
&lt;h3&gt;多项式分布&lt;/h3&gt;</content><category term="ml"></category></entry></feed>